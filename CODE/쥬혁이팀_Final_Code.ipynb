{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17801,"status":"ok","timestamp":1694913041946,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"3vJPt7YblbgX","outputId":"7a2ec83a-bd6f-46e6-953d-1b7c503566ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5128,"status":"ok","timestamp":1694913078472,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"Yey0rGQnfspk"},"outputs":[],"source":["import random\n","import os\n","import pandas as pd\n","import numpy as np\n","from tqdm.auto import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1694913078472,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"Uy-UwlHagFqA","outputId":"00068d80-1e3a-4815-e234-8863555b2e0f"},"outputs":[{"data":{"text/plain":["device(type='cuda', index=0)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n","device"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694913078473,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"e_wYZpGMgFnq"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKqIZa7ZgFlD"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.Y = Y\n","    def __getitem__(self, index):\n","        if self.Y is not None:\n","            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n","        return torch.Tensor(self.X[index])\n","    def __len__(self):\n","        return len(self.X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQbJsifHgFiS"},"outputs":[],"source":["def train(model, optimizer, train_loader, device):\n","    model.to(device)\n","    criterion = nn.MSELoss().to(device)\n","    best_loss = 9999999\n","    best_model = None\n","    for epoch in range(1, 9):\n","        model.train()\n","        train_loss = []\n","        train_mae = []\n","        for X, Y in tqdm(iter(train_loader)):\n","            X = X.to(device)\n","            Y = Y.to(device)\n","            optimizer.zero_grad()\n","            output = model(X)\n","            loss = criterion(output, Y)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss.append(loss.item())\n","        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : []')\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLzQYQScgFf0"},"outputs":[],"source":["def validation(model, val_loader, criterion, device):\n","    model.eval()\n","    val_loss = []\n","\n","    with torch.no_grad():\n","        for X, Y in tqdm(iter(val_loader)):\n","            X = X.to(device)\n","            Y = Y.to(device)\n","\n","            output = model(X)\n","            loss = criterion(output, Y)\n","\n","            val_loss.append(loss.item())\n","    return np.mean(val_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2O2xSADJgFdX"},"outputs":[],"source":["def inference(model, test_loader, device):\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for X in tqdm(iter(test_loader)):\n","            X = X.to(device)\n","            output = model(X)\n","            output = output.cpu().numpy()\n","            predictions.extend(output)\n","\n","    return np.array(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wvYwFR_agFay"},"outputs":[],"source":["def improved_fill_zeros_with_median_and_selective_interpolation_v6(data):\n","    non_zero_data = [value for value in data if value != 0]\n","    if len(non_zero_data) == 0:\n","        return data\n","    median_value = np.median(non_zero_data)\n","    filled_data = data.copy()\n","    non_zero_indices = np.where(data != 0)[0]\n","    if non_zero_indices[0] != 0:\n","        filled_data[:non_zero_indices[0]] = np.linspace(median_value, data[non_zero_indices[0]], non_zero_indices[0]+1)[:-1]\n","    if non_zero_indices[-1] != len(data) - 1:\n","        filled_data[non_zero_indices[-1]+1:] = np.linspace(data[non_zero_indices[-1]], median_value, len(data)-non_zero_indices[-1])[:-1]\n","\n","    for i in range(len(non_zero_indices) - 1):\n","        start, end = non_zero_indices[i], non_zero_indices[i+1]\n","        filled_data[start+1:end] = np.linspace(data[start], data[end], end-start+1)[1:-1]\n","\n","    return filled_data\n","\n","def add_noise_to_interpolated_values_v2(original_data, processed_data, std_dev=0.5):\n","    \"\"\"Add Gaussian noise to the interpolated values in the processed data.\"\"\"\n","    noise = np.where(original_data == 0, np.random.normal(0, std_dev, len(original_data)), 0)\n","    noisy_processed_data = processed_data + noise\n","    return noisy_processed_data\n","\n","def add_bootstrap_noise_to_interpolated_values(original_data, processed_data):\n","    \"\"\"Add bootstrap noise to the interpolated values in the processed data.\"\"\"\n","    non_zero_data = [value for value in original_data if value != 0]\n","    if len(non_zero_data) == 0:\n","        return processed_data\n","    noise_values = np.random.choice(non_zero_data, size=len(original_data), replace=True) - np.median(non_zero_data)\n","    noisy_processed_data = np.where(original_data == 0, processed_data + noise_values, processed_data)\n","    return noisy_processed_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lQAePM2sgFYj"},"outputs":[],"source":["seed_list=[43,44,93779555]\n","seed_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9a0xaGgtgFWM"},"outputs":[],"source":["\n","for seed in seed_list:\n","    CFG = {\n","    'TRAIN_WINDOW_SIZE':28, \n","    'PREDICT_SIZE':21, \n","    'EPOCHS':8,\n","    'LEARNING_RATE':0.001,\n","    'BATCH_SIZE':1024,\n","    'SEED':seed\n","    }\n","\n","    submit = pd.read_csv('./data2/sample_submission.csv')\n","    seed_everything(CFG['SEED'])\n","    for shop in tqdm(range(1,11)):\n","        train_data = pd.read_csv('./data2/train.csv')\n","        shop = str(shop).zfill(2)\n","        train_data = train_data.loc[train_data['쇼핑몰'] == f'S001-000{shop}']\n","        ids = train_data['ID']\n","        date_columns = train_data.columns[7:]\n","\n","        grouped_by_product = train_data.groupby('제품')[date_columns].sum().reset_index()\n","\n","        merged_df = pd.merge(train_data.drop(columns=date_columns), grouped_by_product, on='제품', how='left')\n","        train_data = merged_df.copy()\n","\n","        for i in range(len(train_data)):\n","            value = list(train_data.iloc[i][7:])\n","            interpolated_sales_data = improved_fill_zeros_with_median_and_selective_interpolation_v6(np.array(value))\n","            train_data.iloc[i, 7:] = interpolated_sales_data\n","\n","        numeric_cols = train_data.columns[7:]\n","        min_values = train_data[numeric_cols].min(axis=1)\n","        max_values = train_data[numeric_cols].max(axis=1)\n","        ranges = max_values - min_values\n","        ranges[ranges == 0] = 1\n","        train_data[numeric_cols] = (train_data[numeric_cols].subtract(min_values, axis=0)).div(ranges, axis=0)\n","        scale_min_dict = min_values.to_dict()\n","        scale_max_dict = max_values.to_dict()\n","\n","        def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n","            num_rows = len(data)\n","            window_size = train_size + predict_size\n","            input_data = np.empty((num_rows * (len(data.columns) - window_size + 1), train_size, 1))\n","            target_data = np.empty((num_rows * (len(data.columns) - window_size + 1), predict_size))\n","\n","            for i in tqdm(range(num_rows)):\n","                sales_data = np.array(data.iloc[i, 7:])  # 첫 4개 열을 제외하고 가져옵니다.\n","\n","                for j in range(len(sales_data) - window_size + 1):\n","                    window = sales_data[j : j + window_size]\n","                    input_data[i * (len(data.columns) - window_size + 1) + j] = window[:train_size].reshape(-1, 1)\n","                    target_data[i * (len(data.columns) - window_size + 1) + j] = window[train_size:]\n","\n","            return input_data, target_data\n","\n","        def make_predict_data(data, train_size=CFG['TRAIN_WINDOW_SIZE']):\n","\n","            num_rows = len(data)\n","\n","            input_data = np.empty((num_rows, train_size, 1))\n","\n","            for i in tqdm(range(num_rows)):\n","                sales_data = np.array(data.iloc[i, -train_size:])\n","                input_data[i] = sales_data.reshape(-1, 1)  \n","\n","            return input_data\n","\n","        train_input, train_target = make_train_data(train_data)\n","        test_input = make_predict_data(train_data)\n","        data_len = len(train_input)\n","\n","        train_dataset = CustomDataset(train_input, train_target)\n","        train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n","\n","        class BaseModel(nn.Module):\n","            def __init__(self, input_size=1, hidden_size=512, output_size=CFG['PREDICT_SIZE']):\n","                super(BaseModel, self).__init__()\n","                self.hidden_size = hidden_size\n","                self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n","                self.fc = nn.Sequential(\n","                    nn.Linear(hidden_size*2, hidden_size//2),\n","                    nn.ReLU(),\n","                    nn.Dropout(),\n","                    nn.Linear(hidden_size//2, output_size)\n","                )\n","\n","                self.actv = nn.ReLU()\n","\n","            def forward(self, x):\n","                batch_size = x.size(0)\n","                hidden = self.init_hidden(batch_size, x.device)\n","                lstm_out, hidden = self.lstm(x, hidden)\n","                last_output = lstm_out[:, -1, :]\n","                output = self.actv(self.fc(last_output))\n","                return output.squeeze(1)\n","\n","            def init_hidden(self, batch_size, device):\n","                return (torch.zeros(2, batch_size, self.hidden_size, device=device),\n","                        torch.zeros(2, batch_size, self.hidden_size, device=device))\n","\n","        def train(model, optimizer, train_loader, device):\n","            model.to(device)\n","            criterion = nn.MSELoss().to(device)\n","            best_loss = 9999999\n","            best_model = None\n","\n","            for epoch in range(1, CFG['EPOCHS']+1):\n","                model.train()\n","                train_loss = []\n","                train_mae = []\n","                for X, Y in tqdm(iter(train_loader)):\n","                    X = X.to(device)\n","                    Y = Y.to(device)\n","                    optimizer.zero_grad()\n","                    output = model(X)\n","                    loss = criterion(output, Y)\n","                    loss.backward()\n","                    optimizer.step()\n","\n","                    train_loss.append(loss.item())\n","                print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : []')\n","            return model\n","\n","        model = BaseModel()\n","        optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n","        infer_model = train(model, optimizer, train_loader, device)\n","        test_dataset = CustomDataset(test_input, None)\n","        test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n","        pred = inference(model, test_loader, device)\n","        for idx in range(len(pred)):\n","            pred[idx, :] = pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n","        pred = np.round(pred, 0).astype(int)\n","        submit.loc[submit['ID'].isin(ids.values), '2023-04-25':] = pred\n","    submit.to_csv(f'biLSbyshopping_28_{seed}_.csv',index=False)"]},{"cell_type":"markdown","metadata":{"id":"TA3zDNUlhTtC"},"source":["# LSTFLinear"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLk8CNN0gay6"},"outputs":[],"source":["def improved_fill_zeros_with_median_and_selective_interpolation_v6(data):\n","    non_zero_data = [value for value in data if value != 0]\n","    if len(non_zero_data) == 0:\n","        return data\n","    median_value = np.median(non_zero_data)\n","    filled_data = data.copy()\n","    non_zero_indices = np.where(data != 0)[0]\n","    if non_zero_indices[0] != 0:\n","        filled_data[:non_zero_indices[0]] = np.linspace(median_value, data[non_zero_indices[0]], non_zero_indices[0]+1)[:-1]\n","    if non_zero_indices[-1] != len(data) - 1:\n","        filled_data[non_zero_indices[-1]+1:] = np.linspace(data[non_zero_indices[-1]], median_value, len(data)-non_zero_indices[-1])[:-1]\n","    for i in range(len(non_zero_indices) - 1):\n","        start, end = non_zero_indices[i], non_zero_indices[i+1]\n","        filled_data[start+1:end] = np.linspace(data[start], data[end], end-start+1)[1:-1]\n","    return filled_data\n","\n","def add_noise_to_interpolated_values_v2(original_data, processed_data, std_dev=0.5):\n","    \"\"\"Add Gaussian noise to the interpolated values in the processed data.\"\"\"\n","    noise = np.where(original_data == 0, np.random.normal(0, std_dev, len(original_data)), 0)\n","    noisy_processed_data = processed_data + noise\n","    return noisy_processed_data\n","\n","def add_bootstrap_noise_to_interpolated_values(original_data, processed_data):\n","    \"\"\"Add bootstrap noise to the interpolated values in the processed data.\"\"\"\n","    non_zero_data = [value for value in original_data if value != 0]\n","    if len(non_zero_data) == 0:\n","        return processed_data\n","    noise_values = np.random.choice(non_zero_data, size=len(original_data), replace=True) - np.median(non_zero_data)\n","    noisy_processed_data = np.where(original_data == 0, processed_data + noise_values, processed_data)\n","    return noisy_processed_data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owSp3VqNgawL"},"outputs":[],"source":["train_data = pd.read_csv('train.csv')\n","submit = pd.read_csv('sample_submission.csv')\n","date_columns = train_data.columns[7:]\n","\n","grouped_by_product = train_data.groupby('제품')[date_columns].sum().reset_index()\n","\n","merged_df = pd.merge(train_data.drop(columns=date_columns), grouped_by_product, on='제품', how='left')\n","train_data = merged_df.copy()\n","\n","for i in range(len(train_data)):\n","    value = list(train_data.iloc[i][7:])\n","    interpolated_sales_data = improved_fill_zeros_with_median_and_selective_interpolation_v6(np.array(value))\n","    train_data.iloc[i, 7:] = interpolated_sales_data\n","train_df = train_data.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJ2Lwzakgatt"},"outputs":[],"source":["class LTSF_Linear(torch.nn.Module):\n","    def __init__(self, window_size, forecast_size, individual, feature_size):\n","        super(LTSF_Linear, self).__init__()\n","        self.window_size = window_size\n","        self.forecast_size = forecast_size\n","        self.individual = individual\n","        self.channels = feature_size\n","        if self.individual:\n","            self.Linear = torch.nn.ModuleList()\n","            for i in range(self.channels):\n","                self.Linear.append(torch.nn.Linear(self.window_size, self.forecast_size))\n","        else:\n","            self.Linear = torch.nn.Linear(self.window_size, self.forecast_size)\n","\n","    def forward(self, x):\n","        if self.individual:\n","            output = torch.zeros([x.size(0), self.forecast_size, x.size(2)],dtype=x.dtype).to(x.device)\n","            for i in range(self.channels):\n","                output[:,:,i] = self.Linear[i](x[:,:,i])\n","            x = output\n","        else:\n","            x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n","        return x\n","\n","class Data(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.Y)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.Y[idx]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFXkah5tgarm"},"outputs":[],"source":["def reshape_data(df):\n","    time_series_data = []\n","    for idx, row in df.iterrows():\n","        sales_data = row[7:].values.astype(float)\n","        time_series_data.append(sales_data)\n","    return np.array(time_series_data)\n","\n","def time_slide_df(data, window_size, forecast_size):\n","    data_list = []\n","    dap_list = []\n","    for idx in range(0, len(data) - window_size - forecast_size + 1):\n","        x = data[idx:idx + window_size].reshape(window_size, 1)\n","        y = data[idx + window_size:idx + window_size + forecast_size]\n","        data_list.append(x)\n","        dap_list.append(y)\n","    return np.array(data_list, dtype='float32'), np.array(dap_list, dtype='float32')\n","\n","def create_dataloader(data, window_size, forecast_size, batch_size):\n","    X, Y = time_slide_df(data, window_size, forecast_size)\n","    ds = Data(X, Y)\n","    return DataLoader(ds, batch_size=batch_size, shuffle=True)\n","\n","time_series_data = reshape_data(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nUnjhfAgapM"},"outputs":[],"source":["def time_slide_df(data, window_size, forecast_size):\n","    data_list = []\n","    dap_list = []\n","    for idx in range(0, len(data) - window_size - forecast_size + 1):\n","        x = data[idx:idx + window_size].reshape(window_size, 1)\n","        y = data[idx + window_size:idx + window_size + forecast_size]\n","        data_list.append(x)\n","        dap_list.append(y)\n","    return np.array(data_list, dtype='float32'), np.array(dap_list, dtype='float32')\n","\n","def create_dataloader(data, window_size, forecast_size, batch_size):\n","    X, Y = time_slide_df(data, window_size, forecast_size)\n","    ds = Data(X, Y)\n","    return DataLoader(ds, batch_size=batch_size, shuffle=True)\n","\n","time_series_data = reshape_data(train_df)\n","\n","window_size = 28   # Considering the last 30 days for prediction\n","forecast_size = 21 # Predicting the next 21 days\n","batch_size = 4096\n","epoch_count = 777 #777\n","lr = 0.001\n","min_delta = 0.001\n","patience = 10\n","\n","future_predictions_by_id = {}\n","loss_history = {}\n","\n","for idx, (id_val, data) in tqdm(enumerate(zip(train_df[\"ID\"], time_series_data)), total=len(train_df[\"ID\"])):\n","    mean_ = np.mean(data)\n","    std_ = np.std(data)\n","    if std_ == 0:\n","        standardized_data = data\n","    else:\n","        standardized_data = (data - mean_) / std_\n","    individual_loss_history = []\n","\n","    train_dl = create_dataloader(standardized_data, window_size, forecast_size, batch_size)\n","    best_loss = float('inf')\n","    no_improvement_count = 0\n","\n","    DLinear_model = LTSF_Linear(window_size=window_size, forecast_size=21, individual=False, feature_size=1)\n","    DLinear_model.to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(DLinear_model.parameters(), lr=lr)\n","\n","    for epoch in range(1, epoch_count + 1):\n","        loss_list = []\n","        DLinear_model.train()\n","        for batch_idx, (data, target) in enumerate(train_dl):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = DLinear_model(data)\n","            loss = criterion(output, target.unsqueeze(-1))\n","            loss.backward()\n","            optimizer.step()\n","            loss_list.append(loss.item())\n","\n","        if((epoch % 10) == 0):\n","            avg_loss = np.mean(loss_list)\n","            individual_loss_history.append(avg_loss)\n","            if avg_loss + min_delta < best_loss:\n","                best_loss = avg_loss\n","                no_improvement_count = 0\n","            else:\n","                no_improvement_count += 1\n","                if no_improvement_count >= patience:\n","                    break\n","\n","    loss_history[id_val] = individual_loss_history\n","    last_window_data = torch.tensor(standardized_data[-window_size:]).unsqueeze(0).unsqueeze(-1).float().to(device)\n","    future_prediction = DLinear_model(last_window_data)\n","    if std_ == 0:\n","        future_prediction = future_prediction.squeeze().detach().cpu().numpy()\n","    else:\n","        future_prediction = future_prediction.squeeze().detach().cpu().numpy() * std_ + mean_\n","    future_predictions_by_id[id_val] = future_prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQCw7YGGh7GK"},"outputs":[],"source":["for id_val, predictions in future_predictions_by_id.items():\n","    rounded_predictions = np.round(predictions).astype(np.float64)\n","    submit.loc[submit['ID'] == id_val, '2023-04-25':'2023-05-15'] = rounded_predictions.astype(np.float64)\n","submit.iloc[:, 1:] = submit.iloc[:, 1:].applymap(lambda x: 0 if x < 0 else x)\n","submit = submit.fillna(0)\n","submit.to_csv(f'l_{window_size}_{seed}_.csv',index=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3979,"status":"ok","timestamp":1694913317947,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"NuoXCaeIbTxA"},"outputs":[],"source":["lstm_output_list = ['biLSbyshopping_28_43_.csv', 'biLSbyshopping_28_44_.csv', 'biLSbyshopping_28_93779555_.csv']\n","for lstm_out in lstm_output_list:\n","    lstm_sub = pd.read_csv(f'./{lstm_out}')\n","    lstm_sub_median = lstm_sub.copy()\n","    columns_21_days = lstm_sub_median.columns[1:]\n","    for col in columns_21_days:\n","        lstm_sub_median[col] = lstm_sub[columns_21_days].median(axis=1).astype(int)\n","    for i in [6,7,9]:\n","        df = train_df[train_df['쇼핑몰'] == f'S001-0000{i}']\n","        for j in list(df.index):\n","            lstm_sub.iloc[int(j),1:] = np.round((submit.loc[int(j)][1:].values.astype(np.float64) + lstm_sub.loc[int(j)][1:].values).astype(np.float64) / 2)\n","    filename = lstm_out.split('.')[0]\n","    lstm_sub.to_csv(f'./{filename}avg.csv', index=False)"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":754,"status":"ok","timestamp":1694913348918,"user":{"displayName":"장준보","userId":"12796992458142872403"},"user_tz":-540},"id":"8KLfmoRXrBsl"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","csv_paths = [\"./biLSbyshopping_28_44_avg.csv\",\"./biLSbyshopping_28_93779555_avg.csv\", \"./biLSbyshopping_28_43_avg.csv\"]\n","\n","csv_weights = [0.3334, 0.3333, 0.3333]\n","\n","csv_dfs = [pd.read_csv(path).set_index('ID') for path in csv_paths]\n","\n","columns_21_days = csv_dfs[0].columns\n","\n","ensemble_df = pd.DataFrame(index=csv_dfs[0].index)\n","ensemble_df['ID'] = csv_dfs[0].index\n","\n","ensemble_values = sum(csv_df * weight for csv_df, weight in zip(csv_dfs, csv_weights))\n","ensemble_df[columns_21_days] = np.round(ensemble_values.values).astype(int)\n","\n","ensemble_df.reset_index(drop=True, inplace=True)\n","ensemble_avg_df = ensemble_df.copy()\n","for col in columns_21_days:\n","    ensemble_avg_df[col] = ensemble_df[columns_21_days].mean(axis=1).astype(int)\n","\n","\n","ensemble_avg_df.reset_index(drop=True).to_csv(\"./REAL_FINAL_SUBMIT.csv\", index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}
